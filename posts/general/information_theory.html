<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Nodar Okroshiashvili">
<meta name="dcterms.date" content="2020-07-12">
<meta name="keywords" content="information theory in python, entropy, self information, joint entropy in python, conditional entropy in python">

<title>Basics of Information Theory with Python</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../assets/favicon.ico" rel="icon">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": true,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="Basics of Information Theory with Python">
<meta property="og:description" content="Information flows around us. It’s everywhere. No matter what we have, either it will be some well-known play or painting or just a bunch of numbers or video streams.">
<meta name="twitter:title" content="Basics of Information Theory with Python">
<meta name="twitter:description" content="Information flows around us. It’s everywhere. No matter what we have, either it will be some well-known play or painting or just a bunch of numbers or video streams.">
<meta name="twitter:card" content="summary">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../assets/logo.png" alt="" class="navbar-logo">
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">
 <span class="menu-text">About</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/nodar-okroshiashvili/"><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/Okroshiashvili"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/N_Okroshiashvil"><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Basics of Information Theory with Python</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">General</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Nodar Okroshiashvili </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">July 12, 2020</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a>
  <ul class="collapse">
  <li><a href="#self-information" id="toc-self-information" class="nav-link" data-scroll-target="#self-information">Self-Information</a></li>
  <li><a href="#entropy" id="toc-entropy" class="nav-link" data-scroll-target="#entropy">Entropy</a></li>
  <li><a href="#joint-entropy" id="toc-joint-entropy" class="nav-link" data-scroll-target="#joint-entropy">Joint Entropy</a></li>
  <li><a href="#conditional-entropy" id="toc-conditional-entropy" class="nav-link" data-scroll-target="#conditional-entropy">Conditional Entropy</a></li>
  <li><a href="#mutual-information" id="toc-mutual-information" class="nav-link" data-scroll-target="#mutual-information">Mutual Information</a></li>
  <li><a href="#kullbackleibler-divergence---relative-entropy" id="toc-kullbackleibler-divergence---relative-entropy" class="nav-link" data-scroll-target="#kullbackleibler-divergence---relative-entropy">Kullback–Leibler Divergence - Relative Entropy</a></li>
  <li><a href="#cross-entropy" id="toc-cross-entropy" class="nav-link" data-scroll-target="#cross-entropy">Cross Entropy</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Information flows around us. It’s everywhere. No matter what we have, either it will be some well-known play or painting or just a bunch of numbers or video streams. For computers, all of them are represented by only two digits 0 and 1, and they carry some information. “<strong>Information theory</strong> studies the transmission, processing, extraction, and utilization of information.”<a href="https://en.wikipedia.org/wiki/Information_theory">wikipedia</a> In simple words, with information theory, given different kinds of signals, we try to measure how much information is presented in each of those signals. The theory itself originates from the original work of <a href="https://en.wikipedia.org/wiki/Claude_Shannon">Claude Shannon</a> named <a href="https://en.wikipedia.org/wiki/A_Mathematical_Theory_of_Communication"><em>A Mathematical Theory of Communication</em></a></p>
<p>It will be helpful to see how machine learning and information theory are related. According to “Dive Into Deep Learning” hence d2l considers this relationship to be</p>
<blockquote class="blockquote">
<p>Machine learning aims to extract interesting signals from data and make critical predictions. On the other hand, information theory studies encoding, decoding, transmitting, and manipulating information. As a result, information theory provides a fundamental language for discussing the information processing in machine learned systems.<a href="https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/information-theory.html">source</a></p>
</blockquote>
<p>Information theory is tightly connected to mathematics and statistics. We will see later on how, but before that, it’s worth to say where is used the concepts of information theory in statistics and mathematics. We all know or have heard about <em>random variables</em> that are drawn from some probability distribution. From linear algebra, we also know how to measure the distance between two points, or between two planes. But, how can we measure the distance between two probability distribution? In other words, how similar or dissimilar are these two probability distribution? Information theory gives us the ability to answer this question and quantify the similarity measure between two distributions. Before we continue, let me outline the measurement unit of information theory. Shannon introduced the <strong>bit</strong> as the unit of information. The series of 0 and 1 encode any data. Accordingly, the sequence of binary digits of length <span class="math inline">\(n\)</span> contains <em><span class="math inline">\(n\)</span> bits</em> of information. That has been said, we can review concepts of information theory.</p>
<p>There are a few main concepts in information theory, and I will go through each of them in a detailed manner. First in line is:</p>
<section id="self-information" class="level3">
<h3 class="anchored" data-anchor-id="self-information">Self-Information</h3>
<p>To understand this concept well, I will review two examples—one from statistics and probability and the second from the information theory. Let start with statistics and probability. Imagine we conduct an experiment giving several outcomes with a different probability. For example, rolling the fair dice with uniform probability <span class="math inline">\(\frac{1}{6}\)</span> of returning numbers from 1 to 6. Now, consider three outcomes, defined as <span class="math inline">\(A=\{outcome \leq 6\}\)</span> <span class="math inline">\(B=\{outcome is odd\}\)</span>, and <span class="math inline">\(C=\{outcome=1\}\)</span> over probability space <span class="math inline">\(\Omega\)</span>, which in turn contains all the outcomes. <strong>Self-information</strong>, sometimes stated as <strong>information content</strong> or <strong>surprisal</strong> indicates how much unlikely the event <span class="math inline">\(A\)</span>, or <span class="math inline">\(B\)</span>, or <span class="math inline">\(C\)</span> is, how much surprised we are by observing either event. Here is the question: How can we convert probability <span class="math inline">\(p\)</span> of an event into a number of bits? Claude Shannon gave us the formula for that:</p>
<p><span class="math display">\[
I(X) = - \log_2(p)
\]</span></p>
<p>For our three events, <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span>, and <span class="math inline">\(C\)</span> the self-information or surprisal is the following:</p>
<p><span class="math display">\[
I(A) = - \log_2(1) = 0
\]</span></p>
<p><span class="math display">\[
I(B) = - \log_2(\frac{3}{6}) = 1
\]</span></p>
<p><span class="math display">\[
I(C) = - \log_2(\frac{1}{6}) = 2.58
\]</span></p>
<p>From an information theory perspective, if we have a series of binary digits of the length <span class="math inline">\(n\)</span>, then the probability of getting 0 or 1 is <span class="math inline">\(\frac{1}{2^{n}}\)</span>. According to Shannon, self-information is the bits of information we receive from observing the event <span class="math inline">\(X\)</span>. Let <span class="math inline">\(X\)</span> be the following code: <code>0101</code>, then its information content is <strong>4 bits</strong> according to our formula:</p>
<p><span class="math display">\[
I(X) = I(0101) = - \log_2(\frac{1}{2^{4}}) = 4
\]</span></p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> self_information(p):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>np.log2(p)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>self_information(<span class="dv">1</span> <span class="op">/</span> <span class="dv">2</span><span class="op">**</span><span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<pre><code>4.0</code></pre>
</div>
</div>
<p>The main takeaway here is that if a particular event has 100% probability, its self-information is <span class="math inline">\(-\log_2(1) = 0\)</span>, meaning that it does not carry any information, and we have no surprise at all. Whereas, if the probability would be close to zero, or we can effectively say it’s zero, then self-information is <span class="math inline">\(-\log_2(0) = \infty\)</span>. This implies that the rare events have high surprisal or high information content.</p>
<p>We see that information content only measures the information of a single event. To generalize this notion for any discrete and/or continues event, we will get the idea of <strong>Entropy</strong>.</p>
</section>
<section id="entropy" class="level3">
<h3 class="anchored" data-anchor-id="entropy">Entropy</h3>
<p>If we have any random variable <span class="math inline">\(X\)</span>, whether it will be a discrete or continuous and <span class="math inline">\(X\)</span> follows a probability distribution <span class="math inline">\(P\)</span> with <code>p.d.f</code> if it’s continuous or <code>p.m.f</code> if it’s discrete. Can we calculate the average value of <span class="math inline">\(X\)</span>? Yes, we can. From statistics, the formula of the average or a.k.a expectation is</p>
<p><span class="math display">\[
\mathbb E(X) = \sum_{i=1}^{k} x_{i} \cdot p_{i}
\]</span></p>
<p>Where <span class="math inline">\(x_{i}\)</span> is one particular event with its probability <span class="math inline">\(p_{i}\)</span>. The same is in information theory. The <strong>Entropy</strong> of a random variable <span class="math inline">\(X\)</span> is the expectation of its self-information, given by:</p>
<p><span class="math display">\[
H(X) = - \sum_{i} p_{i} \log_{2} p_{i}
\]</span></p>
<p>In Python it looks the following:</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># np.nansum return the sum of NaNs. Treats them as zeros.</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> entropy(p):</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> np.nansum(<span class="op">-</span>p <span class="op">*</span> np.log2(p))</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> out</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>entropy(np.array([<span class="fl">0.1</span>, <span class="fl">0.5</span>, <span class="fl">0.1</span>, <span class="fl">0.3</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>1.6854752972273346</code></pre>
</div>
</div>
<p>Here, we only consider one random variable, <span class="math inline">\(X\)</span>, and its expected surprisal. What if we have two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>? How can we measure their joint information content? In other words, we are interested what information is included in <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> compared to each separately. Here comes the <strong>Joint Entropy</strong></p>
</section>
<section id="joint-entropy" class="level3">
<h3 class="anchored" data-anchor-id="joint-entropy">Joint Entropy</h3>
<p>To review this concept let me introduce two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> and they follow the probability distribution denoted by <span class="math inline">\(p_{X}(x)\)</span> and <span class="math inline">\(p_Y(y)\)</span>, respectively. <span class="math inline">\((X, Y)\)</span> has joint probability <span class="math inline">\(p_{X, Y}(x, y)\)</span>. The <strong>Joint Entropy</strong> hence is defined as:</p>
<p><span class="math display">\[
H(X, Y) = - \sum_{x} \sum_{y} p_{X, Y}(x, y) \log_{2} p_{X, Y}(x, y)
\]</span></p>
<p>Here are two important facts. If <span class="math inline">\(X = Y\)</span> this implies that <span class="math inline">\(H(X,Y) = H(X) = H(Y)\)</span> and if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, then <span class="math inline">\(H(X, Y) = H(X) + H(Y)\)</span>.</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> joint_entropy(p_xy):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> np.nansum(<span class="op">-</span>p_xy <span class="op">*</span> np.log2(p_xy))</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> out</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>joint_entropy(np.array([[<span class="fl">0.1</span>, <span class="fl">0.5</span>, <span class="fl">0.8</span>], [<span class="fl">0.1</span>, <span class="fl">0.3</span>, <span class="fl">0.02</span>]]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>2.0558948969327187</code></pre>
</div>
</div>
<p>As we see, joint entropy indicates the amount of information in the pair of two random variables. What if we are interested to know how much information is contained, say in <span class="math inline">\(Y\)</span> but not in <span class="math inline">\(X\)</span>?</p>
</section>
<section id="conditional-entropy" class="level3">
<h3 class="anchored" data-anchor-id="conditional-entropy">Conditional Entropy</h3>
<p>The <strong>conditional entropy</strong> is used to measure the relationship between variables. The following formula gives this measurement:</p>
<p><span class="math display">\[
H(Y \mid X) = - \sum_{x} \sum_{y} p(x, y) \log_{2} p(y \mid x)
\]</span></p>
<p>Let investigate how conditional entropy is related to entropy and joint entropy. Using the above formula, we can conclude that:</p>
<p><span class="math display">\[
H(Y \mid X) = H(X, Y) - H(X)
\]</span></p>
<p>meaning that the information contained in <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span> equals information jointly contained in <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> minus the amount of information only contained in <span class="math inline">\(X\)</span>.</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> conditional_entropy(p_xy, p_x):</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    p_y_given_x <span class="op">=</span> p_xy <span class="op">/</span> p_x</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> np.nansum(<span class="op">-</span>p_xy <span class="op">*</span> np.log2(p_y_given_x))</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> out</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>conditional_entropy(np.array([[<span class="fl">0.1</span>, <span class="fl">0.5</span>], [<span class="fl">0.2</span>, <span class="fl">0.3</span>]]), np.array([<span class="fl">0.2</span>, <span class="fl">0.8</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>0.8635472023399721</code></pre>
</div>
</div>
<p>Knowing conditional entropy means knowing the amount of information contained in <span class="math inline">\(Y\)</span> but not in <span class="math inline">\(X\)</span>. Now let see how much information is shared between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p>
</section>
<section id="mutual-information" class="level3">
<h3 class="anchored" data-anchor-id="mutual-information">Mutual Information</h3>
<p>To find the <strong>mutual information</strong> between two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, let start the process by finding all the information in both <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> together and then subtract the part which is not shared. The information both in <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is <span class="math inline">\(H(X, Y)\)</span>. Subtracting two conditional entropies gives:</p>
<p><span class="math display">\[
I(X, Y) = H(X, Y) - H(Y \mid X) − H(X \mid Y)
\]</span></p>
<p>This means that we have to subtract the information only contained in <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> to all the information at hand. This relationship is perfectly described by this picture.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="mutual_information.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">mutual_information</figcaption><p></p>
</figure>
</div>
<p>The concept of mutual information likewise correlation coefficient, allow us to measure the linear relationship between two random variables as well as the amount of maximum information shared between them.</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mutual_information(p_xy, p_x, p_y):</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    p <span class="op">=</span> p_xy <span class="op">/</span> (p_x <span class="op">*</span> p_y)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> np.nansum(p_xy <span class="op">*</span> np.log2(p))</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> out</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>mutual_information(np.array([[<span class="fl">0.1</span>, <span class="fl">0.5</span>], [<span class="fl">0.1</span>, <span class="fl">0.3</span>]]), np.array([<span class="fl">0.2</span>, <span class="fl">0.8</span>]), np.array([[<span class="fl">0.75</span>, <span class="fl">0.25</span>]]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>0.7194602975157967</code></pre>
</div>
</div>
<p>As in the case of the correlation coefficient, mutual information has some notable properties:</p>
<ul>
<li>Mutual information is symmetric</li>
<li>Mutual information is non-negative</li>
<li><span class="math inline">\(I(X, Y) = 0\)</span> iff <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent</li>
</ul>
<p>We can interpret the mutual information <span class="math inline">\(I(X, Y)\)</span> as the average amount of surprisal by seeing two outcomes happening together compared to what we would expect if they were independent.</p>
</section>
<section id="kullbackleibler-divergence---relative-entropy" class="level3">
<h3 class="anchored" data-anchor-id="kullbackleibler-divergence---relative-entropy">Kullback–Leibler Divergence - Relative Entropy</h3>
<p>I asked the question about measuring the distance between two probability distributions. The time has come to answer this question precisely. If we have random variable <span class="math inline">\(X\)</span> which follows probability distributin <span class="math inline">\(P\)</span> and has <code>p.d.f</code> or <code>p.m.f</code> <span class="math inline">\(p(x)\)</span>. Imagine we estimated <span class="math inline">\(P\)</span> with other probability distribution <span class="math inline">\(Q\)</span>, which in turn has <code>p.d.f</code> or <code>p.m.f</code> <span class="math inline">\(q(x)\)</span>. The distance between thse two probability distribution is measured by <strong>Kullback–Leibler (KL) Divergence</strong>:</p>
<p><span class="math display">\[
D_{\mathrm{KL}}(P\|Q) = E_{x \sim P} \left[ \log \frac{p(x)}{q(x)} \right]
\]</span></p>
<p>The lower value of the <span class="math inline">\(KL\)</span> divergence, the closer our estimate is to the actual distribution.</p>
<ul>
<li>The KL divergence is non-symmetric or equivalently, <span class="math inline">\(D_{\mathrm{KL}}(P\|Q) \neq D_{\mathrm{KL}}(Q\|P), \text{ if } P \neq Q\)</span></li>
<li>The KL divergence is non-negative or equivalently, <span class="math inline">\(D_{\mathrm{KL}}(P\|Q) \geq 0\)</span></li>
</ul>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> kl_divergence(p, q):</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>    kl <span class="op">=</span> p <span class="op">*</span> np.log2(p <span class="op">/</span> q)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> np.nansum(kl)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.<span class="bu">abs</span>(out)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> np.random.normal(<span class="dv">1</span>, <span class="dv">2</span>, size<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>q <span class="op">=</span> np.random.normal(<span class="dv">1</span>, <span class="dv">2</span>, size<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>kl_divergence(p, q)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/var/folders/mz/3thvm62j52l8lpr5sllrt6rh0000gn/T/ipykernel_16290/409365545.py:2: RuntimeWarning: invalid value encountered in log2
  kl = p * np.log2(p / q)</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="13">
<pre><code>803.4821687459835</code></pre>
</div>
</div>
</section>
<section id="cross-entropy" class="level3">
<h3 class="anchored" data-anchor-id="cross-entropy">Cross Entropy</h3>
<p>To understand <strong>Cross-Entropy</strong>, let me use the example from the KL divergence part. Now, imagine we perform classification tasks, where <span class="math inline">\(y\)</span> is the true label, and <span class="math inline">\(\hat{y}\)</span> is estimated label by our model. <strong>Cross-Entropy</strong> denoted by <span class="math inline">\(\mathrm{CE}(y, \hat{y})\)</span> is used as a objective function in many classification tasks in deep learning. The formula is the following:</p>
<p><span class="math display">\[
\mathrm{CE} (P, Q) = H(P) + D_{\mathrm{KL}}(P\|Q)
\]</span></p>
<p>The two terms on the right-hand side are self-information and KL divergence. <span class="math inline">\(P\)</span> is the distribution of the true labels, and <span class="math inline">\(Q\)</span> is the distribution of the estimated labels. As we are only interested in knowing how far we are from the actual label and <span class="math inline">\(H(P)\)</span> is also given, the above formula is reduced to minimize only the second term (KL divergence) at the right-hand side. Hence, we have</p>
<p><span class="math display">\[
\mathrm{CE}(\mathbf{y}, \hat{\mathbf{y}}) = - \sum_{i=1}^n \sum_{j=1}^k y_{ij} \log_{2}{p_{\theta} (y_{ij}  \mid  \mathbf{x}_i)}
\]</span></p>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> cross_entropy(y_hat, y):</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>    ce <span class="op">=</span> <span class="op">-</span>np.log(y_hat[<span class="bu">range</span>(<span class="bu">len</span>(y_hat)), y])</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> ce.mean()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> np.array([<span class="dv">0</span>, <span class="dv">2</span>])</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>preds <span class="op">=</span> np.array([[<span class="fl">0.3</span>, <span class="fl">0.6</span>, <span class="fl">0.1</span>], [<span class="fl">0.2</span>, <span class="fl">0.3</span>, <span class="fl">0.5</span>]])</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>cross_entropy(preds, labels)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="15">
<pre><code>0.9485599924429406</code></pre>
</div>
</div>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>By reviewing these concepts from the information theory, we have some rough sense of how it’s related to the statistics and mathematics and is used in machine learning and deep learning. There is much more to discover, and that’s up to you how far you want to go. Moreover, even interesting is how information theory is related to the coding theory, in gambling and musical composition.</p>
<section id="references" class="level4">
<h4 class="anchored" data-anchor-id="references">References</h4>
<ul>
<li><a href="https://d2l.ai/index.html">Dive Into Deep Learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/Information_theory">Information theory</a></li>
</ul>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
      <div class="nav-footer-center">Copyright 2023, Nodar Okroshiashvili</div>
  </div>
</footer>



</body></html>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>linear_algebra_advance_part_ii</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../assets/favicon.ico" rel="icon">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": true,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../assets/logo.png" alt="" class="navbar-logo">
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">
 <span class="menu-text">About</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/nodar-okroshiashvili/"><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/Okroshiashvili"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/N_Okroshiashvil"><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">



<p>Title: Advance Linear Algebra with Python - Part II Date: 2019-03-05 12:14 Category: Mathematics Tags: Linear Algebra, Advance Topics Keywords: advance linear algebra, matrix decompositions in python, linear algebra advances in python, advance linear algebra for machine learning Author: Nodar Okroshiashvili Summary: This is the second part of advance linear algebra with Python</p>
<p>This is the fourth and last post in blog series about linear algebra.</p>
<ol type="1">
<li>Introduction</li>
<li>Basics of linear algebra</li>
<li>Intermediate linear algebra</li>
<li><strong>Advances in linear algebra</strong>: Part I and <strong>Part II</strong></li>
</ol>
<p>This is the continuation of the part one and in this post I will introduce you the following topics:</p>
<ul>
<li><a href="#matrix-decompositions">Matrix Decompositions</a>
<ul>
<li><a href="#cholesky-decomposition">Cholesky Decomposition</a></li>
<li><a href="#qr-decomposition">QR Decomposition</a></li>
<li><a href="#eigendecomposition">Eigendecomposition</a></li>
<li><a href="#singular-value-decomposition">Singular Value Decomposition</a></li>
<li><a href="#inverse-of-a-square-full-rank-matrix">Inverse of a Square Full Rank Matrix</a></li>
</ul></li>
<li><a href="#numerical-representation">Numerical Representation</a>
<ul>
<li><a href="#cholesky-decomposition-1">Cholesky Decomposition</a></li>
<li><a href="#qr-decomposition-1">QR Decomposition</a></li>
<li><a href="#eigendecomposition-1">Eigendecomposition</a></li>
<li><a href="#singular-value-decomposition-1">Singular Value Decomposition</a></li>
<li><a href="#inverse-of-a-square-full-rank-matrix-1">Inverse of a Square Full Rank Matrix</a></li>
</ul></li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#references">References</a></li>
</ul>
<section id="matrix-decompositions" class="level2">
<h2 class="anchored" data-anchor-id="matrix-decompositions">Matrix Decompositions</h2>
<p>In linear algebra, matrix decomposition or matrix factorization is a factorization of a matrix into a product of matrices. Factorizing a matrix means that we want to find a product of matrices that is equal to the initial matrix. These techniques have a wide variety of uses and consequently, there exist several types of decompositions. Below, I will consider some of them, mostly applicable to machine learning or deep learning.</p>
<section id="cholesky-decomposition" class="level3">
<h3 class="anchored" data-anchor-id="cholesky-decomposition">Cholesky Decomposition</h3>
<p>The Cholesky Decomposition is the factorization of a given <strong>symmetric</strong> square matrix <span class="math inline">\(A\)</span> into the product of a lower triangular matrix, denoted by <span class="math inline">\(L\)</span> and its transpose <span class="math inline">\(L^{T}\)</span>. This decomposition is named after French artillery officer <a href="https://en.wikipedia.org/wiki/Andr%C3%A9-Louis_Cholesky">Andre-Louis Cholesky</a>. The formula is:</p>
<p><span class="math display">\[
A =
LL^{T}
\]</span></p>
<p>For rough sense, let <span class="math inline">\(A\)</span> be</p>
<p><span class="math display">\[
A =
\begin{bmatrix}
a_{11} &amp; a_{12} &amp; a_{13} \\
a_{21} &amp; a_{22} &amp; a_{23} \\
a_{31} &amp; a_{32} &amp; a_{33}
\end{bmatrix}
\]</span></p>
<p>Then we can represent <span class="math inline">\(A\)</span> as</p>
<p><span class="math display">\[
A = LL^{T} =
\begin{bmatrix}
l_{11} &amp; 0 &amp; 0 \\
l_{21} &amp; l_{22} &amp; 0 \\
l_{31} &amp; l_{32} &amp; l_{33}
\end{bmatrix}
\cdot
\begin{bmatrix}
l_{11} &amp; l_{12} &amp; l_{13} \\
0 &amp; l_{22} &amp; l_{23} \\
0 &amp; 0 &amp; a_{33}
\end{bmatrix} =
\begin{bmatrix}
l_{11}^{2} &amp; l_{21}l_{11} &amp; l_{31}l_{11} \\
l_{21}l_{11} &amp; l_{21}^{2} + l_{22}^{2} &amp; l_{31}l_{21} + l_{32}l_{22} \\
l_{31}l_{11} &amp; l_{31}l_{21} + l_{32}l_{22} &amp; l_{31}^{2} + l_{32}^{2} + l_{33}^2
\end{bmatrix}
\]</span></p>
<p>The diagonal elements of matrix <span class="math inline">\(L\)</span> can be calculated by the following formulas:</p>
<p><span class="math display">\[
l_{11} = \sqrt{a_{11}}
\quad \quad
l_{22} = \sqrt{a_{22} - l_{21}^{2}}
\quad \quad
l_{33} = \sqrt{a_{33} - (l_{31}^{2} + l_{32}{2})}
\]</span></p>
<p>And in general, for diagonal elements of the matrix <span class="math inline">\(L\)</span> we have:</p>
<p><span class="math display">\[
l_{kk} =
\sqrt{a_{kk} - \sum_{j = 1}^{k - 1}l_{kj}^{2}}
\]</span></p>
<p>For the elements below the main diagonal, <span class="math inline">\(l_{ik}\)</span> where <span class="math inline">\(i &gt; k\)</span>, the formulas are</p>
<p><span class="math display">\[
l_{21} = \frac{1}{l_{11}}a_{21}
\quad \quad
l_{31} = \frac{1}{l_{11}}a_{31}
\quad \quad
l_{32} = \frac{1}{l_{22}}(a_{32} - l_{31}l_{21})
\]</span></p>
<p>And the general formula is</p>
<p><span class="math display">\[
l_{ik} =
\frac{1}{l_{kk}}\Big(a_{ik} - \sum_{j = 1}^{k - 1}l_{ij}l_{kj}\Big)
\]</span></p>
<p>Messy formulas! Consider a numerical example to see what happen under the hood. We have a matrix <span class="math inline">\(A\)</span></p>
<p><span class="math display">\[
A =
\begin{bmatrix}
25 &amp; 15 &amp; -5 \\
15 &amp; 18 &amp; 0 \\
-5 &amp; 0 &amp; 11
\end{bmatrix}
\]</span></p>
<p>According to the above formulas, let find a lower triangular matrix <span class="math inline">\(L\)</span>. We have</p>
<p><span class="math display">\[
l_{11} = \sqrt{a_{11}} = \sqrt{25} = 5
\quad \quad
l_{22} = \sqrt{a_{22} - l_{21}^{2}} = \sqrt{18 - 3^{2}} = 3
\quad \quad
l_{33} = \sqrt{a_{33} - (l_{31}^{2} + l_{32}^{2})} = \sqrt{11 - ((-1)^{2} + 1^{2})} = 3
\]</span></p>
<p>Seems, we have missing non-diagonal elements, which are</p>
<p><span class="math display">\[
l_{21} = \frac{1}{l_{11}}a_{21} = \frac{1}{5}15 = 3
\quad \quad
l_{31} = \frac{1}{l_{11}}a_{31} = \frac{1}{5}(-5) = -1
\quad \quad
l_{32} = \frac{1}{l_{22}}(a_{32} - l_{31}l_{21}) = \frac{1}{3}(0 - (-1)\cdot 3) = 1
\]</span></p>
<p>So, our matrix <span class="math inline">\(L\)</span> is</p>
<p><span class="math display">\[
L =
\begin{bmatrix}
5 &amp; 0 &amp; 0 \\
3 &amp; 3 &amp; 0 \\
-1 &amp; 1 &amp; 3
\end{bmatrix}
\quad \quad
L^{T} =
\begin{bmatrix}
5 &amp; 3 &amp; -1 \\
0 &amp; 3 &amp; 1 \\
0 &amp; 0 &amp; 3
\end{bmatrix}
\]</span></p>
<p>Multiplication of this matrices is up to you.</p>
</section>
<section id="qr-decomposition" class="level3">
<h3 class="anchored" data-anchor-id="qr-decomposition">QR Decomposition</h3>
<p>QR decomposition is another type of matrix factorization, where a given <span class="math inline">\(m \times n\)</span> matrix <span class="math inline">\(A\)</span> is decomposed into two matrices, <span class="math inline">\(Q\)</span> which is orthogonal matrix, which in turn means that <span class="math inline">\(QQ^{T} = Q^{T}Q = I\)</span> and the inverse of <span class="math inline">\(Q\)</span> equal to its transpose, <span class="math inline">\(Q^{T} = Q^{-1}\)</span>, and <span class="math inline">\(R\)</span> which is upper triangular matrix. Hence, the formula is given by</p>
<p><span class="math display">\[
A =
QR
\]</span></p>
<p>As <span class="math inline">\(Q\)</span> is an orthogonal matrix, there are three methods to find <span class="math inline">\(Q\)</span>, one is <a href="https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process">Gramm-Schmidt Process</a>, second is <a href="https://en.wikipedia.org/wiki/Householder_transformation">Householder Transformation</a>, and third is <a href="https://en.wikipedia.org/wiki/Givens_rotation">Givens Rotation</a>. These methods are out of the scope of this blog post series and hence I’m going to explain all of them in separate blog posts. Consequently, there is no calculation besides python code in numerical representation section.</p>
</section>
<section id="eigendecomposition" class="level3">
<h3 class="anchored" data-anchor-id="eigendecomposition">Eigendecomposition</h3>
<p>Here is the question. What’s the usage of eigenvalues and eigenvectors? Besides other usages, they help us to perform matrix decomposition and this decomposition is called eigendecomposition or spectral decomposition. In the case of the eigendecomposition, we decompose the initial matrix into the product of its eigenvectors and eigenvalues by the following formula:</p>
<p><span class="math display">\[
A = Q \Lambda Q^{-1}
\]</span></p>
<p><span class="math inline">\(A\)</span> is <span class="math inline">\(n\times n\)</span> square matrix, <span class="math inline">\(Q\)</span> is the matrix whose columns are the eigenvectors, which in turn are linearly independent and <span class="math inline">\(\Lambda\)</span> is diagonal matrix of eigenvalues of <span class="math inline">\(A\)</span> and these eigenvalues are not necessarily distinct.</p>
<p>To see the detailed steps of this decomposition, consider the aforementioned example of the matrix <span class="math inline">\(A\)</span> for which we already found eigenvalues and eigenvectors.</p>
<p><span class="math display">\[
A =
\begin{bmatrix}
2 &amp; 0 &amp; 0 \\
0 &amp; 3 &amp; 4 \\
0 &amp; 4 &amp; 9
\end{bmatrix}
\quad
Q =
\begin{bmatrix}
0 &amp; 1 &amp; 0 \\
-2 &amp; 0 &amp; 1 \\
1 &amp; 0 &amp; 2
\end{bmatrix}
\quad
\Lambda =
\begin{bmatrix}
1 &amp; 0 &amp; 0 \\
0 &amp; 2 &amp; 0 \\
0 &amp; 0 &amp; 11
\end{bmatrix}
\quad
Q^{-1} =
\begin{bmatrix}
0 &amp; -0.4 &amp; 0.2 \\
1 &amp; 0 &amp; 0 \\
0 &amp; 0.2 &amp; 0.4
\end{bmatrix}
\]</span></p>
<p>We have all the matrices and now take matrix multiplication according to the above formula. Particularly, multiply <span class="math inline">\(Q\)</span> by <span class="math inline">\(\Lambda\)</span> and by <span class="math inline">\(Q^{-1}\)</span>. We have to get original matrix <span class="math inline">\(A\)</span></p>
<p>Furthermore, if matrix <span class="math inline">\(A\)</span> is a real symmetric matrix, then eigendecomposition can be performed by the following formula:</p>
<p><span class="math display">\[
A = Q \Lambda Q^{T}
\]</span></p>
<p>The only difference between this formula and above formula is that the matrix <span class="math inline">\(A\)</span> is <span class="math inline">\(n\times n\)</span> real symmetric square matrix and instead of taking the inverse of eigenvector matrix we take the transpose of it. Moreover, for a real symmetric matrix, eigenvectors corresponding to different eigenvalues are orthogonal. Consider the following example:</p>
<p><span class="math display">\[
A =
\begin{bmatrix}
6 &amp; 2 \\
2 &amp; 3
\end{bmatrix}
\]</span></p>
<p>The matrix is symmetric because of the original matrix equal to its transpose, <span class="math inline">\(A = A^{T}\)</span></p>
<p>Its eigenvalues are <span class="math inline">\(\lambda_{1} = 7\)</span> and <span class="math inline">\(\lambda_{2} = 2\)</span> and corresponding eigenvectors are</p>
<p><span class="math display">\[
v_{\lambda_{1}} =
\begin{bmatrix}
0.89442719 \\
0.4472136
\end{bmatrix}
\quad
v_{\lambda_{2}} =
\begin{bmatrix}
-0.4472136 \\
0.89442719
\end{bmatrix}
\]</span></p>
<p>And in this set up, matrices <span class="math inline">\(Q\)</span>, <span class="math inline">\(\Lambda\)</span> and <span class="math inline">\(Q^{T}\)</span> are the following:</p>
<p><span class="math display">\[
Q =
\begin{bmatrix}
0.89442719 &amp; -0.4472136 \\
0.4472136 &amp; 0.89442719 \\
\end{bmatrix}
\quad
\Lambda =
\begin{bmatrix}
7 &amp; 0 \\
0 &amp; 2 \\
\end{bmatrix}
\quad
Q^{T} =
\begin{bmatrix}
0.89442719 &amp; 0.4472136 \\
-0.4472136 &amp; 0.89442719 \\
\end{bmatrix}
\]</span></p>
<p>Taking matrix product gives initial matrix <span class="math inline">\(A\)</span>.</p>
<p>To verify all of this calculation see Python code below.</p>
<p><strong>Eigendecomposition cannot be used for non-square matrices. Below, we will see the Singular Value Decomposition (SVD) which is another way of decomposing matrices. The advantage of the SVD is that you can use it also with non-square matrices.</strong></p>
</section>
<section id="singular-value-decomposition" class="level3">
<h3 class="anchored" data-anchor-id="singular-value-decomposition">Singular Value Decomposition</h3>
<p>Singular Value Decomposition (SVD) is another way of matrix factorization. It is the generalization of the eigendecomposition. In this context, generalization means that eigendecomposition is applicable only for square <span class="math inline">\(n \times n\)</span> matrices, while Singular Value Decomposition (SVD) is applicable for any <span class="math inline">\(m \times n\)</span> matrices.</p>
<p>SVD for a <span class="math inline">\(m \times n\)</span> matrix <span class="math inline">\(A\)</span> is computed by the following formula:</p>
<p><span class="math display">\[
A = U \ D \ V^{T}
\]</span></p>
<p>Where, <span class="math inline">\(U\)</span>’s columns are <em>left singular vectors</em> of <span class="math inline">\(A\)</span>, <span class="math inline">\(V\)</span>’s columns are <em>right singular vectors</em> of <span class="math inline">\(A\)</span> and <span class="math inline">\(D\)</span> is a diagonal matrix, not necessarily square matrix, containing <strong>singular values</strong> of <span class="math inline">\(A\)</span> on main diagonal. Singular values of <span class="math inline">\(m \times n\)</span> matrix <span class="math inline">\(A\)</span> are the <strong>square roots of the eigenvalues</strong> of <span class="math inline">\(A^{T}A\)</span>, which is a square matrix. If our initial matrix <span class="math inline">\(A\)</span> is square or <span class="math inline">\(n \times n\)</span> then singular values <strong>coincide</strong> eigenvalues. Moreover, all of these defines the path towards eigendecomposition. Let see how this path is defined.</p>
<p>Matrices, <span class="math inline">\(U\)</span>, <span class="math inline">\(D\)</span>, and <span class="math inline">\(V\)</span> can be found by transforming <span class="math inline">\(A\)</span> into a square matrix and computing eigenvalues and eigenvectors of this transformed matrix. This transformation is done by multiplying <span class="math inline">\(A\)</span> by its transpose <span class="math inline">\(A^{T}\)</span>. After that, matrices <span class="math inline">\(U\)</span>, <span class="math inline">\(D\)</span> and <span class="math inline">\(V\)</span> are the following:</p>
<ul>
<li><p><span class="math inline">\(U\)</span> corresponds to the eigenvectors of <span class="math inline">\(AA^{T}\)</span></p></li>
<li><p><span class="math inline">\(V\)</span> corresponds to eigenvectors of <span class="math inline">\(A^{T}A\)</span></p></li>
<li><p><span class="math inline">\(D\)</span> corresponds to eigenvalues, either <span class="math inline">\(AA^{T}\)</span> or <span class="math inline">\(A^{T}A\)</span>, which are the same</p></li>
</ul>
<p>Theory almost always seems confusing. Consider a numerical example and Python code below for clarification.</p>
<p>Let our initial matrix <span class="math inline">\(A\)</span> be:</p>
<p><span class="math display">\[
A =
\begin{bmatrix}
0 &amp; 1 &amp; 0 \\
\sqrt{2} &amp; 2 &amp; 0 \\
0 &amp; 1 &amp; 1
\end{bmatrix}
\]</span></p>
<p>Here, to use SVD first we need to find <span class="math inline">\(AA^{T}\)</span> and <span class="math inline">\(A^{T}A\)</span>.</p>
<p><span class="math display">\[
AA^{T} =
\begin{bmatrix}
2 &amp; 2 &amp; 2 \\
2 &amp; 6 &amp; 2 \\
2 &amp; 2 &amp; 2
\end{bmatrix}
\quad
A^{T}A =
\begin{bmatrix}
2 &amp; 2\sqrt{2} &amp; 0 \\
2\sqrt{2} &amp; 6 &amp; 2 \\
0 &amp; 2 &amp; 2
\end{bmatrix}
\]</span></p>
<p>In the next step, we have to find eigenvalues and eigenvectors for <span class="math inline">\(AA^{T}\)</span> and <span class="math inline">\(A^{T}A\)</span>. The characteristic polynomial is</p>
<p><span class="math display">\[
-\lambda^{3} + 10\lambda^2 - 16\lambda
\]</span></p>
<p>with roots equal to <span class="math inline">\(\lambda_{1} = 8\)</span>, <span class="math inline">\(\lambda_{2} = 2\)</span>, and <span class="math inline">\(\lambda_{3} = 0\)</span>. Note that these eigenvalues are the same for the <span class="math inline">\(A^{T}A\)</span>. We need singular values which are square root from eigenvalues. Let denote them by <span class="math inline">\(\sigma\)</span> such as <span class="math inline">\(\sigma_{1} = \sqrt{8} = 2\sqrt{2}\)</span>, <span class="math inline">\(\sigma_{2} = \sqrt{2}\)</span> and <span class="math inline">\(\sigma_{3} = \sqrt{0} = 0\)</span>. We now can construct diagonal matrix of singular values:</p>
<p><span class="math display">\[
D =
\begin{bmatrix}
2\sqrt{2} &amp; 0 &amp; 0 \\
0 &amp; \sqrt{2} &amp; 0 \\
0 &amp; 0 &amp; 0
\end{bmatrix}
\]</span></p>
<p>Now we have to find matrices <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span>. We have everything what we need. First find eigenvectors of <span class="math inline">\(AA^{T}\)</span> for <span class="math inline">\(\lambda_{1} = 8\)</span>, <span class="math inline">\(\lambda_{2} = 2\)</span>, and <span class="math inline">\(\lambda_{3} = 0\)</span>, which are the following:</p>
<p><span class="math display">\[
U_{1} =
\begin{bmatrix}
\frac{1}{\sqrt{6}}\\
\frac{2}{\sqrt{6}} \\
\frac{1}{\sqrt{6}}
\end{bmatrix}
\quad
U_{2} =
\begin{bmatrix}
-\frac{1}{\sqrt{3}}\\
\frac{1}{\sqrt{3}} \\
-\frac{1}{\sqrt{3}}
\end{bmatrix}
\quad
U_{3} =
\begin{bmatrix}
\frac{1}{\sqrt{2}}\\
0 \\
-\frac{1}{\sqrt{2}}
\end{bmatrix}
\]</span></p>
<p>Note that eigenvectors are normalized.</p>
<p>As we have eigenvectors, our <span class="math inline">\(U\)</span> matrix is:</p>
<p><span class="math display">\[
U =
\begin{bmatrix}
\frac{1}{\sqrt{6}} &amp; -\frac{1}{\sqrt{3}} &amp; \frac{1}{\sqrt{2}}\\
\frac{2}{\sqrt{6}} &amp; \frac{1}{\sqrt{3}} &amp; 0 \\
\frac{1}{\sqrt{6}} &amp; -\frac{1}{\sqrt{3}} &amp; -\frac{1}{\sqrt{2}}
\end{bmatrix}
\]</span></p>
<p>In the same fashion, we can find matrix <span class="math inline">\(V\)</span>, which is:</p>
<p><span class="math display">\[
V =
\begin{bmatrix}
\frac{1}{\sqrt{6}} &amp; \frac{1}{\sqrt{3}} &amp; \frac{1}{\sqrt{2}}\\
\frac{3}{\sqrt{12}} &amp; 0 &amp; -\frac{1}{2} \\
\frac{1}{\sqrt{12}} &amp; -\frac{2}{\sqrt{6}} &amp; \frac{1}{2}
\end{bmatrix}
\]</span></p>
<p>According to the formula we have</p>
<p><span class="math display">\[
A = U \ D \ V^{T} =
\begin{bmatrix}
\frac{1}{\sqrt{6}} &amp; -\frac{1}{\sqrt{3}} &amp; \frac{1}{\sqrt{2}}\\
\frac{2}{\sqrt{6}} &amp; \frac{1}{\sqrt{3}} &amp; 0 \\
\frac{1}{\sqrt{6}} &amp; -\frac{1}{\sqrt{3}} &amp; -\frac{1}{\sqrt{2}}
\end{bmatrix}
\cdot
\begin{bmatrix}
2\sqrt{2} &amp; 0 &amp; 0 \\
0 &amp; \sqrt{2} &amp; 0 \\
0 &amp; 0 &amp; 0
\end{bmatrix}
\cdot
\begin{bmatrix}
\frac{1}{\sqrt{6}} &amp; \frac{1}{\sqrt{3}} &amp; \frac{1}{\sqrt{2}}\\
\frac{3}{\sqrt{12}} &amp; 0 &amp; -\frac{1}{2} \\
\frac{1}{\sqrt{12}} &amp; -\frac{2}{\sqrt{6}} &amp; \frac{1}{2}
\end{bmatrix}
^{T} = A
\]</span></p>
</section>
<section id="inverse-of-a-square-full-rank-matrix" class="level3">
<h3 class="anchored" data-anchor-id="inverse-of-a-square-full-rank-matrix">Inverse of a Square Full Rank Matrix</h3>
<p>Here, I want to present one more way to find the inverse of a matrix and show you one more usage of eigendecomposition. Let’s get started. If a matrix <span class="math inline">\(A\)</span> can be eigendecomposed and it has no any eigenvalue equal to zero, then this matrix has the inverse and this inverse is given by:</p>
<p><span class="math display">\[
A^{-1} =
Q \Lambda^{-1} Q^{-1}
\]</span></p>
<p>Matrices, <span class="math inline">\(Q\)</span>, and <span class="math inline">\(\Lambda\)</span> are already known for us. Consider an example:</p>
<p><span class="math display">\[
A =
\begin{bmatrix}
1 &amp; 2 \\
4 &amp; 3
\end{bmatrix}
\]</span></p>
<p>Its eigenvalues are <span class="math inline">\(\lambda_{1} = -1\)</span> and <span class="math inline">\(\lambda_{2} = 5\)</span> and eigenvectors are:</p>
<p><span class="math display">\[
v_{\lambda_{1}} =
\begin{bmatrix}
-0.70710678 \\
0.70710678
\end{bmatrix}
\quad
v_{\lambda_{2}} =
\begin{bmatrix}
0.4472136 \\
-0.89442719
\end{bmatrix}
\]</span></p>
<p>Let calculate the inverse of <span class="math inline">\(A\)</span></p>
<p><span class="math display">\[
A^{-1} = Q \Lambda^{-1} Q^{-1} =
\begin{bmatrix}
-0.70710678 &amp; -0.4472136 \\
0.70710678 &amp; -0.89442719
\end{bmatrix}
\cdot
\begin{bmatrix}
-1 &amp; -0 \\
0 &amp; 0.2
\end{bmatrix}
\cdot
\begin{bmatrix}
-0.94280904 &amp; 0.47140452 \\
-0.74535599 &amp; -0.74535599
\end{bmatrix} =
\begin{bmatrix}
-0.6 &amp; 0.4 \\
0.8 &amp; -0.2
\end{bmatrix}
\]</span></p>
</section>
</section>
<section id="numerical-representation" class="level2">
<h2 class="anchored" data-anchor-id="numerical-representation">Numerical Representation</h2>
<section id="cholesky-decomposition-1" class="level3">
<h3 class="anchored" data-anchor-id="cholesky-decomposition-1">Cholesky Decomposition</h3>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array([[<span class="dv">25</span>,<span class="dv">15</span>,<span class="op">-</span><span class="dv">5</span>],</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">15</span>,<span class="dv">18</span>,<span class="dv">0</span>],</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>              [<span class="op">-</span><span class="dv">5</span>,<span class="dv">0</span>,<span class="dv">11</span>]])</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Cholesky decomposition, find lower triangular matrix L</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>L <span class="op">=</span> np.linalg.cholesky(A)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Take transpose</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>L_T <span class="op">=</span> np.transpose(L)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Check if it's correct</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>A <span class="op">==</span> np.dot(L, L_T)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>array([[ True,  True,  True],
       [ True,  True,  True],
       [ True,  True,  True]])</code></pre>
</section>
<section id="qr-decomposition-1" class="level3">
<h3 class="anchored" data-anchor-id="qr-decomposition-1">QR Decomposition</h3>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array([[<span class="dv">12</span>,<span class="op">-</span><span class="dv">51</span>,<span class="dv">4</span>],</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">6</span>,<span class="dv">167</span>,<span class="op">-</span><span class="dv">68</span>],</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>              [<span class="op">-</span><span class="dv">4</span>,<span class="dv">24</span>,<span class="op">-</span><span class="dv">41</span>]])</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="co"># QR decomposition</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>Q, R <span class="op">=</span> np.linalg.qr(A)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Q ="</span>, Q, sep<span class="op">=</span><span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"R ="</span>, R, sep<span class="op">=</span><span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"A = QR"</span>, np.dot(Q,R), sep<span class="op">=</span><span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>Q =
[[-0.85714286  0.39428571  0.33142857]
 [-0.42857143 -0.90285714 -0.03428571]
 [ 0.28571429 -0.17142857  0.94285714]]

R =
[[ -14.  -21.   14.]
 [   0. -175.   70.]
 [   0.    0.  -35.]]

A = QR
[[ 12. -51.   4.]
 [  6. 167. -68.]
 [ -4.  24. -41.]]</code></pre>
</section>
<section id="eigendecomposition-1" class="level3">
<h3 class="anchored" data-anchor-id="eigendecomposition-1">Eigendecomposition</h3>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Eigendecomposition for non-symmetric matrix</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array([[<span class="dv">2</span>,<span class="dv">0</span>,<span class="dv">0</span>],</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">0</span>,<span class="dv">3</span>,<span class="dv">4</span>],</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">0</span>,<span class="dv">4</span>,<span class="dv">9</span>]])</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>eigenvalues1, eigenvectors1 <span class="op">=</span> np.linalg.eig(A)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Form diagonal matrix from eigenvalues</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>L1 <span class="op">=</span> np.diag(eigenvalues1)</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Separate eigenvector matrix and take its inverse</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>Q1 <span class="op">=</span> eigenvectors1</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>inv_Q <span class="op">=</span> np.linalg.inv(Q1)</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>B <span class="op">=</span> np.dot(np.dot(Q1,L1),inv_Q)</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Check if B equal to A</span></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Decomposed matrix B:"</span>)</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(B)</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Numpy produces normalized eigenvectors and don't be confused with my calculations above</span></span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Eigendecomposition for symmetric matrix</span></span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>C <span class="op">=</span> np.array([[<span class="dv">6</span>,<span class="dv">2</span>],[<span class="dv">2</span>,<span class="dv">3</span>]])</span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>eigenvalues2, eigenvectors2 <span class="op">=</span> np.linalg.eig(C)</span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Eigenvalues</span></span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a>L2 <span class="op">=</span> np.diag(eigenvalues2)</span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a><span class="co"># Eigenvectors</span></span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a>Q2 <span class="op">=</span> eigenvectors2</span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a>Q2_T <span class="op">=</span> Q2.T</span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a>D <span class="op">=</span> np.dot(np.dot(Q2,L2),Q2.T)</span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-54"><a href="#cb5-54" aria-hidden="true" tabindex="-1"></a><span class="co"># Check if D equal to C</span></span>
<span id="cb5-55"><a href="#cb5-55" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb5-56"><a href="#cb5-56" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Decomposed matrix D:"</span>)</span>
<span id="cb5-57"><a href="#cb5-57" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(D)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>Decomposed matrix B:
[[2. 0. 0.]
 [0. 3. 4.]
 [0. 4. 9.]]

Decomposed matrix D:
[[6. 2.]
 [2. 3.]]</code></pre>
</section>
<section id="singular-value-decomposition-1" class="level3">
<h3 class="anchored" data-anchor-id="singular-value-decomposition-1">Singular Value Decomposition</h3>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>np.set_printoptions(suppress<span class="op">=</span><span class="va">True</span>) <span class="co"># Suppress scientific notation</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array([[<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>],</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>              [np.sqrt(<span class="dv">2</span>),<span class="dv">2</span>,<span class="dv">0</span>],</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>]])</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>U, D, V <span class="op">=</span> np.linalg.svd(A)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"U ="</span>, U)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"D ="</span>, D)</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"V ="</span>, V)</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>B <span class="op">=</span> np.dot(U, np.dot(np.diag(D), V))</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"B ="</span>, B)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>U = [[-0.32099833  0.14524317 -0.93587632]
 [-0.87192053 -0.43111301  0.23215547]
 [-0.36974946  0.8905313   0.26502706]]

D = [2.75398408 1.09310654 0.46977627]

V = [[-0.44774472 -0.8840243  -0.13425984]
 [-0.55775521  0.15876626  0.81467932]
 [ 0.69888038 -0.43965249  0.56415592]]

B = [[ 0.          1.         -0.        ]
 [ 1.41421356  2.          0.        ]
 [ 0.          1.          1.        ]]</code></pre>
</section>
<section id="inverse-of-a-square-full-rank-matrix-1" class="level3">
<h3 class="anchored" data-anchor-id="inverse-of-a-square-full-rank-matrix-1">Inverse of a Square Full Rank Matrix</h3>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array([[<span class="dv">1</span>,<span class="dv">2</span>],</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">4</span>,<span class="dv">3</span>]])</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Eigenvalues and Eigenvectors</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>L, Q <span class="op">=</span> np.linalg.eig(A)</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Diagonal eigenvalues</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>L <span class="op">=</span> np.diag(L)</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Inverse</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>inv_L <span class="op">=</span> np.linalg.inv(L)</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Inverse of igenvector matrix</span></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>inv_Q <span class="op">=</span> np.linalg.inv(Q)</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the inverse of A</span></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>inv_A <span class="op">=</span> np.dot(Q,np.dot(inv_L,inv_Q))</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the inverse</span></span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"The inverse of A is"</span>)</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(inv_A)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>The inverse of A is
[[-0.6  0.4]
 [ 0.8 -0.2]]</code></pre>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>In conclusion, my aim was to make linear algebra tutorials which are in absence, while learning machine learning or deep learning. Particularly, existing materials either are pure mathematics books which cover lots of unnecessary(actually they are necessary) things or machine learning books which assume that you already have some linear algebra knowledge. The series starts from very basic and at the end explains some advanced topics. I can say that I tried my best to filter the materials and only explained the most relevant linear algebra topics for machine learning and deep learning.</p>
<p>Based on my experience, these tutorials are not enough to master the concepts and all intuitions but the journey should be continuous. Meaning, that you have to practice more and more.</p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<p><a href="https://rosettacode.org/wiki/Cholesky_decomposition">Cholesky Decomposition</a></p>
<p><a href="https://en.wikipedia.org/wiki/Matrix_decomposition">Matrix Decomposition</a></p>
<p><a href="http://math.mit.edu/~gs/linearalgebra/">Introduction To Linear Algebra</a></p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
      <div class="nav-footer-center">Copyright 2023, Nodar Okroshiashvili</div>
  </div>
</footer>



</body></html>